configfile: "../config/config.yaml"


#################### SET UP Variables #######################################
import os
import pandas as pd
import numpy as np
from scripts.snake_functions import *

core_genera=["Bombilactobacillus" , "Commensalibacter", "Lactobacillus","Bifidobacterium","Gilliamella",  "Frischella", "Snodgrassella",  "Bartonella", "Apibacter"]

################################################################################
######################### ALL #################################################
################################################################################
type_of=["B", "P"]

GENOMES,= glob_wildcards("../results/pangenomes/B/annotations/genes_faa/{genome}_genes.faa")
GENUSES,= glob_wildcards("../results/pangenomes/B/mOTUpan/{genus}_pan.tsv")
REF_GENOMES= glob_wildcards("../results/reference_db_filtered/dereplicated_genomes_filtered/{genome}")

rule all:
    input:
        #expand("../results/host_assigniation/IPHOP/{sample}_iphop", sample=config["samples"])
        #expand("../results/inStrain/compare_B/compare_B/compare_{genome}", genome=REF_GENOMES.genome),
        #expand("../results/pangenomes/B/mOTUpan/" + "{genus}" + "_pan.tsv", genus=core_genera)
        #expand("../results/pangenomes/B/defense_systems/{genome}", type=type_of , genome=GENOMES),
        expand("../results/inStrain/profile_{type}/{sample}{type}_profile/", type=type_of, sample=config["sam_names"])
        # expand("../results/vMAG_binning/vRhyme_bins/{asmbl}/", asmbl=config["samples"]),
        # "../results/vMAG_binning/summary_tables/all_viral_contigs_metadata.tsv",
        # "../results/assembly/viral_contigs/dereplicated/all_genomes_alignment",
        # "../results/vMAG_binning/summary_tables/all_viral_contigs_dereplication.tsv",
        # "../results/Vcontact2/vCONTACT_results",
        # "../results/host_assigniation/summary_table/phage_host.tsv",
        # "../results/viral_classification/vcontact/vcontact_ntw_filtered.tsv",
        # expand("../results/assembly/viral_contigs/QC_filtered/{sample}_viral_contigs_filtered.fasta", sample=config["samples"])


##################################################################

################################################################################
########################### DATA VALIDATION ####################################
################################################################################
# The following pipeline is used to ascertain that the metagenomics samples
# are composed of what we expect (viruses in the virome fraction, bacteria in the
# bacteriome one)
################################################################################
################################################################################


################################### Concat Lanes ###################################

#Concatenate raw reads from the same sample that were sequenced on different lanes
rule concat_lanes:
    input:
        R1s=get_direction_r1,
        R2s=get_direction_r2
    output:
        R1_concat=temp("../scratch_link/concat_raw_reads/{sample}_R1_concat.fastq.gz"), # these files go in scratch because they can be quickly recreated if needed
        R2_concat=temp("../scratch_link/concat_raw_reads/{sample}_R2_concat.fastq.gz")
    threads: 2
    log:
        "logs/data_validation/lane_concatenation/{sample}_concat.log"
    resources:
        account = "pengel_beemicrophage",
        runtime= "02:00:00"
    shell:
        "cat {input.R1s} > {output.R1_concat}; cat {input.R2s} > {output.R2_concat}; "

################################### Kraken2 ###################################

# This rule takes the concatenated raw reads files and runs them against the krakend db
rule run_Krakern2:
    input:
        R1="../scratch_link/concat_raw_reads/{sample}_R1_concat.fastq.gz",
        R2="../scratch_link/concat_raw_reads/{sample}_R2_concat.fastq.gz",
        db="resources/default_DBs/230228_costum_kraken2db_new"
    output:
        tab=temp("../results/data_validation/kraken2_output/{sample}_kraken2_report.kraken"),
        rep=temp("../results/data_validation/kraken2_output/Reports/{sample}_kraken2_report")
    conda:
        "envs/Kraken2.yaml"
    threads: 8
    log:
        "logs/data_validation/kraken2/run/{sample}_kraken2.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "01:00:00"
    shell:
        "kraken2 --use-names --threads {threads} \
         --db {input.db} \
         --fastq-input --report {output.rep}  --gzip-compressed \
         --paired {input.R1} {input.R2} \
         > {output.tab}"

# This rule parses the kraken output for further analyes
rule parse_kraken_report:
    input:
        "scripts/data_validation/parse_kraken_report.py", # if you change the script, the rule runs again
        expand("../results/data_validation/kraken2_output/Reports/{sample}_kraken2_report", sample=config["samples"])
    output:
        "../results/data_validation/kraken2_output/Summary/all_samples_report.txt"
    threads: 2
    params:
        "../results/data_validation/kraken2_output/"
    log:
        "logs/data_validation/kraken2/parsing/report_parser_kraken2.log"
    resources:
        account = "pengel_beemicrophage",
        runtime= "00:15:00"
    script:
        "scripts/data_validation/parse_kraken_report.py"


############################# QC and Trimming ##################################

# This rule does a fastQC on the raw reads
rule fastQC_PreTrimming:
    input:
        R1=get_direction_r1,
        R2=get_direction_r2
    output:
        temp(directory("../results/data_validation/QC/preTrimming/QC_{sample}/"))
    threads: 2
    log:
        "logs/data_validation/QC/{sample}_QC.log"
    conda:
        "envs/fastqc.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 6000,
        runtime= "01:00:00"
    shell:
        "mkdir -p {output}; "
        "fastqc -o {output} {input.R1};"
        "fastqc -o {output} {input.R2};"

# This rule runs the trimming of the raw reads
rule rawreads_trimming:
    input:
        R1="../scratch_link/concat_raw_reads/{sample}_R1_concat.fastq.gz",
        R2="../scratch_link/concat_raw_reads/{sample}_R2_concat.fastq.gz"
    output:
        R1_paired="../data/trimmed_reads/{sample}_R1_paired.fastq.gz",
        R2_paired="../data/trimmed_reads/{sample}_R2_paired.fastq.gz",
        R1_unpaired="../data/trimmed_reads/{sample}_R1_unpaired.fastq.gz",
        R2_unpaired="../data/trimmed_reads/{sample}_R2_unpaired.fastq.gz"
    threads: 8
    params:
        nextera="../data/reference_assemblies/short_RefSeqs/NexteraPE-PE.fa",
        q=28,
        min_length=40
    log:
        "logs/data_validation/trimming/{sample}_trimming.log"
    conda:
        "envs/trimmomatic.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 6000,
        runtime= "03:00:00"
    shell:
        "trimmomatic PE -phred33 -threads {threads} {input.R1} {input.R2} \
         {output.R1_paired} {output.R1_unpaired} {output.R2_paired} {output.R2_unpaired} \
         ILLUMINACLIP:{params.nextera}:2:30:10 \
         LEADING:{params.q} TRAILING:{params.q} MINLEN:{params.min_length}"

# this rule does a post trimming fast QC
rule fastQC_PostTrimming:
    input:
        R1="../data/trimmed_reads/{sample}_R1_paired.fastq.gz",
        R2="../data/trimmed_reads/{sample}_R2_paired.fastq.gz"
    output:
        temp(directory("../results/data_validation/QC/postTrimming/QC_{sample}/"))
    threads: 2
    log:
        "logs/data_validation/QC/{sample}_postQC.log"
    conda:
        "envs/fastqc.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 6000,
        runtime= "01:00:00"
    shell:
        "mkdir -p {output}; "
        "fastqc -o {output} {input.R1} {input.R2}"

rule parse_fastQC:
    input:
        preT=expand("../results/data_validation/QC/preTrimming/QC_{sample}/", sample=config["samples"]),
        postT=expand("../results/data_validation/QC/postTrimming/QC_{sample}/", sample=config["samples"])
    output:
        "../results/data_validation/QC/Summary/fastQC_summary.txt"
    threads: 2
    log:
        "logs/data_validation/QC/summarize_fastQC.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 6000,
        runtime= "00:20:00"
    script:
        "scripts/data_validation/parse_fastqc_output.py"


############################### Host Filtering #################################

# This rule uses bbsplit to map remove reads from the honeybee genome and/or the human genomes
rule host_filtering:
    input:
        R1="../data/trimmed_reads/{sample}_R1_paired.fastq.gz",
        R2="../data/trimmed_reads/{sample}_R2_paired.fastq.gz"
    output: # I don't need the sam of the mapping so I delete them immediatly
        unmapped_R1="../data/host_filtered_reads/{sample}_R1_HF.fastq.gz", # these are the filtered reads
        unmapped_R2="../data/host_filtered_reads/{sample}_R2_HF.fastq.gz",
        refstats="../results/data_validation/host_filtering/HF_mappings_stats/{sample}_refstats.out"
    conda:
        "envs/bwa_mapping.yaml"
    threads: 25
    params:
        ref_Amel="../data/reference_assemblies/A_mellifera/GCF_003254395.2_Amel_HAv3.1_genomic_concat.fna",
        ref_Hsap="../data/reference_assemblies/H_sapiens/GCF_000001405.40_GRCh38.p14_genomic.fna.gz",
        dir="../results/data_validation/host_filtering/discarded/{sample}_discarded/",
        xmx="50g"
    log:
        "logs/data_validation/HF/{sample}_HF.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "08:00:00"
    shell:
        "bbsplit.sh in1={input.R1} in2={input.R2} ref={params.ref_Amel},{params.ref_Hsap} \
        basename={params.dir}/{wildcards.sample}_HF_discarded_%.sam \
        refstats={output.refstats} rebuild=t nodisk=t \
        outu1={output.unmapped_R1} outu2={output.unmapped_R2} nzo=f -Xmx{params.xmx} threads={threads}"

# This rule parses the refstats output of the filering
rule parse_filtering_refstats:
    input:
        files=expand("../results/data_validation/host_filtering/HF_mappings_stats/{sample}_refstats.out", sample=config["samples"])
    output:
        "../results/data_validation/host_filtering/HF_mappings_stats/HF_refstats.txt"
    threads: 2
    log:
        "logs/data_validation/HF/HF_refstats_parsing.log"
    params:
        file1="../results/data_validation/host_filtering/HF_mappings_stats/file1.txt",
        file2="../results/data_validation/host_filtering/HF_mappings_stats/file2.txt",
        tmp="../results/data_validation/host_filtering/HF_mappings_stats/tmp.txt",
        sams=expand("{sample}", sample=config["samples"])
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 500,
        runtime= "00:30:00"
    shell:
        "echo -e 'sample\tname\tperc_unambiguousReads\tunambiguousMB\tperc_ambiguousReads\tambiguousMB\tunambiguousReads\tambiguousReads\tassignedReads\tassignedBases' > {output}; "
        "tail -n +2 -q {input.files} > {params.file1}; "
        "printf '%s\n' {params.sams} > {params.file2}; "
        "awk '{{for(i=0;i<2;i++)print}}' {params.file2} > {params.tmp}; "
        "paste -d '\t' {params.tmp} {params.file1} >> {output}"


############################### count_reads ##################################
# After trimming and host filtering the reads, I wanna know how much I lost in
# Terms of reads and bases


# this rules returns a table of read count before and after trimming
rule count_reads_qc:
    input:
        preT="../scratch_link/concat_raw_reads/{sample}_R1_concat.fastq.gz",
        postT="../data/trimmed_reads/{sample}_R1_paired.fastq.gz",
        postF="../data/host_filtered_reads/{sample}_R1_HF.fastq.gz"
    output:
        temp("../results/data_validation/QC/{sample}_read_count.txt")
    log:
        "logs/data_validation/QC/read_count/{sample}_read_count.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 4000,
        runtime= "00:30:00"
    shell:
        "touch {output}; "
        "(./scripts/data_validation/count_reads.sh {input.preT} {input.postT} {input.postF} {output})2>{log}"

rule count_reads_summary:
    input:
        sams=expand("../results/data_validation/QC/{sample}_read_count.txt", sample=config["samples"])
    output:
        "../results/data_validation/QC/Summary/read_count.txt"
    log:
        "logs/data_validation/QC/read_count/summary_read_count.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 4000,
        runtime= "00:30:00"
    shell:
        "(awk 'FNR!=NR && FNR==1 {{next}} 1' {input.sams} > {output})2>{log}"


################################################################################
###############################  mOTUS  ########################################
################################################################################
# mOTUs is more has more taxonomical resolution than kraken, so once the reads
# are all cleaned and filtered, I run mOTUs to obtain a genus-level composition
# of the remaining reads (for the bacterial samples)
################################################################################
################################################################################

# this rule runs the motu profiling
# it is better to launch this rule with one sample first and then all the others,
# mOTUs db download doesn't handle weell multiple files trying to download it and
# access it at the same time and the jobs might fail.
rule run_motus:
    input:
        reads1 = "../data/host_filtered_reads/{sample}_R1_HF.fastq.gz",
        reads2 = "../data/host_filtered_reads/{sample}_R2_HF.fastq.gz"
    output:
        motus_temp = temp("../results/data_validation/motus_output/map/{sample}_map.motus")
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 500000,
        runtime= "02:30:00"
    threads: 24
    log:
        "logs/data_validation/motus/{sample}_motus.log"
    conda:
        "envs/motus-env.yaml"
    shell:
        "motus downloadDB; " # just leave it here to be safe - it warns and continues
        "motus profile -f {input.reads1} -r {input.reads2} -n {wildcards.sample} -o {output.motus_temp} -t {threads}"

# this roule run the mouts counting of the marker genes that map to the datbase
rule run_motus_count:
    input:
        reads1 = "../data/host_filtered_reads/{sample}_R1_HF.fastq.gz",
        reads2 = "../data/host_filtered_reads/{sample}_R2_HF.fastq.gz"
    output:
        motus_temp = temp("../results/data_validation/motus_output/count/{sample}_count.motus")
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 500000,
        runtime= "02:30:00"
    threads: 24
    log:
        "logs/data_validation/motus/{sample}_motus.log"
    conda:
        "envs/motus-env.yaml"
    shell:
        "motus downloadDB; " # just leave it here to be safe - it warns and continues
        "motus profile -f {input.reads1} -r {input.reads2} -c -n {wildcards.sample} -o {output.motus_temp} -t {threads}"

rule merge_motus:
    input:
        motus_temp = expand("../results/data_validation/motus_output/map/{sample}_map.motus", sample=config["samples"]),
        motus_count= expand("../results/data_validation/motus_output/count/{sample}_count.motus", sample=config["samples"])
    output:
        motus_merged = "../results/data_validation/motus_output/Summary/samples_merged_map.motus",
        mouts_merged_count = "../results/data_validation/motus_output/Summary/samples_merged_count.motus"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 500000,
        runtime= "02:00:00"
    threads: 12
    log:
        "logs/data_validation/motus/merge_motus.log"
    conda:
        "envs/motus-env.yaml"
    shell:
        "motus merge -a bee -i $(echo \"{input.motus_temp}\" | sed -e 's/ /,/g' ) > {output.motus_merged}; "
        "motus merge -a bee -c -i $(echo \"{input.motus_count}\" | sed -e 's/ /,/g' ) > {output.mouts_merged_count}"

rule parse_motus:
    input:
        motus_tab = "../results/data_validation/motus_output/Summary/samples_merged_map.motus",
        motus_count = "../results/data_validation/motus_output/Summary/samples_merged_count.motus"
    output:
        "../results/data_validation/motus_output/Summary/motus_combined.txt"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 50000,
        runtime= "03:30:00"
    log:
        "logs/data_validation/motus/parse_motus.log"
    conda:
        "envs/base_R_env.yaml"
    script:
        "scripts/data_validation/parse_motus.R"

################################################################################
################################## MAGS ########################################
################################################################################
# This part of the pipleine takes takes host filtered reads and reconstructs MAGS
# This part of the pipeline is divided in several steps:
#   (1): The host-filtered reads are assembled using metaspades
#   (2): backmapping where the read of each sample are mapped against the assembly
#        of every other sample
#   (3): scaffold are binned into MAGS in functon of their coverage across samples
#        using metabat2
#   (4): MAGs are QCed, filtered in function of completeness and contamination,
#        and taxonomically classified
################################################################################
################################################################################

##################################### (1) ######################################
# the following rules use spades to assemble the metagenomes of every sample.
# then assembly are filtered in function of length and coverage


##################################### (1.1) ######################################
# the following rules use metaSPades to assemble the metagenomes of every sample.
rule assemble_host_filtered:
    input:
        R1 = "../data/host_filtered_reads/{sample}_R1_HF.fastq.gz",
        R2 = "../data/host_filtered_reads/{sample}_R2_HF.fastq.gz"
    output:
        scaffolds="../results/assembly/HF_assembly/metaspades/{sample}_metaspades/{sample}_contigs.fasta",
        graph = temp("../results/assembly/HF_assembly/metaspades/{sample}_metaspades/{sample}_assembly_graph.fastg"),
        spades_log = temp("../results/assembly/HF_assembly/metaspades/{sample}_metaspades/{sample}_spades.log")
    params:
        memory_limit = 200,
        dir=directory("../scratch_link/assembly/HF_assembly/{sample}_metaspades/")
    threads: 40
    resources:
        account="pengel_beemicrophage",
        mem_mb= 250000,
        runtime= "24:00:00"
    log:
        "logs/assembly/HF/{sample}_assemble_HF.log"
    benchmark:
        "logs/assembly/HF/{sample}_assemble_HF.benchmark"
    conda:
        "envs/assembly.yaml"
    shell:
        "spades.py --meta --pe1-1 {input.R1} --pe1-2 {input.R2} -o {params.dir} -k 21,33,55,77,99,127 -m {params.memory_limit} -t {threads}; "
        "mv {params.dir}/contigs.fasta {output.scaffolds}; "
        "mv {params.dir}/assembly_graph.fastg {output.graph}; "
        "mv {params.dir}/spades.log {output.spades_log}; "

##################################### (1.2) ######################################
# the following rules use metaviralSPades to assemble the metagenomes of every sample.

rule assemble_metaviralspades:
    input:
        R1 = "../data/host_filtered_reads/{sample}_R1_HF.fastq.gz",
        R2 = "../data/host_filtered_reads/{sample}_R2_HF.fastq.gz"
    output:
        scratch_dir=temp(directory("../scratch_link/assembly/metaviralspades/{sample}_metaviralspades/")),
        contigs="../results/assembly/HF_assembly/metaviralspades/{sample}_metaviralspades/{sample}_contigs.fasta"
    params:
        memory_limit = 200
    threads: 40
    resources:
        account="pengel_beemicrophage",
        mem_mb= 250000,
        runtime= "07:00:00"
    log:
        "logs/assembly/metaviralspades/{sample}_assemble.log"
    log:
        "logs/assembly/metaviralspades/{sample}_assemble.benchmark"
    conda:
        "envs/assembly.yaml"
    shell:
        "spades.py --metaviral --pe1-1 {input.R1} --pe1-2 {input.R2}  -o {output.scratch_dir} -k 21,33,55,77,99,127 -m {params.memory_limit} -t {threads}; "
        "mv {output.scratch_dir}/contigs.fasta {output.contigs}; "


assemblers=["metaspades", "metaviralspades"]


rule parse_HF_assemblies:
    input:
        scaffolds = "../results/assembly/HF_assembly/{assembler}/{sample}_{assembler}/{sample}_contigs.fasta"
    output:
        parsed_scaffold="../results/assembly/HF_assembly/{assembler}/{sample}_{assembler}/{sample}_contigs_parsed.fasta",
        filt_tab=temp("../results/assembly/HF_assembly/{sample}_contigs_{assembler}_tab.txt")
    params:
        length_t = 1000,
        cov_t = 1
    conda: "envs/mags_env.yaml"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 2000,
        runtime= "00:30:00"
    log:
        "logs/assembly/HF/{assembler}/{sample}_parse_{assembler}_assembly_HF.log"
    script:
        "scripts/assembly/parse_spades_metagenome.py"

rule aggragate_filtering_tabs:
    input:
        expand("../results/assembly/HF_assembly/{sample}_contigs_{{assembler}}_tab.txt", sample=config["samples"])
    output:
        all_filt_tab="../results/assembly/HF_assembly/all_HFassemblies_{assembler}_summary_tab.txt"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 2000,
        runtime= "00:15:00"
    log:
        "logs/assembly/HF/{assembler}/aggragate_{assembler}_filterig.log"
    shell:
        "echo -e 'sample\tcontig\tlength\tcov\taccepted' > {output}; "
        "cat {input} >> {output}"

##################################### (2) ######################################
# The following rules perform a mapping of all samples against all samples
# to obtain a depth profile for the assembly of each sample

rule build_backmapping_index:
    input:
        ref="../results/assembly/HF_assembly/metaspades/{sam_name2}B_metaspades/{sam_name2}B_contigs_parsed.fasta"
    output:
        "../results/assembly/HF_assembly/{sam_name2}B_metaspades/{sam_name2}B_index"
    conda:
        "envs/map_env.yaml"
    threads: 15
    params:
        basename="{sam_name2}B_contigs_parsed",
    log:
        "logs/MAGs/backmapping/indexing/{sam_name2}_build_bowtie_index.log"
    resources:
        account= "pengel_beemicrophage",
        mem_mb= 20000,
        runtime= "01:00:00"
    shell:
        "mkdir -p {output}; "
        "bowtie2-build {input.ref} {output}/{params.basename} --threads {threads}"

rule backmapping: # risk of running out of buffer memory, run fewer jobs at the time (with --jobs 50 works; maybe one could increase a bit)
    input:
        assembly = "../results/assembly/HF_assembly/{sam_name2}B_metaspades/{sam_name2}B_index", # we bin only bacteria
        R1 = "../data/host_filtered_reads/{sam_name}B_R1_HF.fastq.gz",
        R2 = "../data/host_filtered_reads/{sam_name}B_R2_HF.fastq.gz"
    output:
        sam=temp("../scratch_link/MAG_binning/backmapping/{sam_name}B/{sam_name}B_mapped_to_{sam_name2}B_contigs_parsed.sam")
    resources:
        account="pengel_beemicrophage",
        runtime="72:00:00",
        mem_mb = 10000
    params:
        basename="{sam_name2}B_contigs_parsed"
    threads: 15
    conda: "envs/map_env.yaml"
    log:
        "logs/MAGs/backmapping/{sam_name}B_backmapping_to{sam_name2}asmbl.log"
    benchmark: "logs/MAGs/backmapping/{sam_name}B_backmapping_to{sam_name2}asmbl.benchmark"
    shell:
        "bowtie2 -x {input.assembly}/{params.basename} -1 {input.R1} -2 {input.R2} -S {output.sam} --threads {threads}"


rule backmapping_depths:
    input:
        sam= "../scratch_link/MAG_binning/backmapping/{sam_name}B/{sam_name}B_mapped_to_{sam_name2}B_contigs_parsed.sam"
    output:
        bam= temp("../scratch_link/MAG_binning/backmapping/{sam_name}B/{sam_name}B_mapped_to_{sam_name2}B_contigs_parsed.bam"),
        depth= temp("../scratch_link/MAG_binning/backmapping/{sam_name}B/{sam_name}B_mapped_to_{sam_name2}B_contigs_parsed.depth")
    resources:
        account="pengel_beemicrophage",
        runtime="10:00:00",
        mem_mb = 10000
    params:
        tmp="../scratch_link/"
    threads: 15
    conda: "envs/sam_env.yaml"
    log:
        "logs/MAGs/backmapping/depth/{sam_name}B_{sam_name2}_depth.log"
    benchmark:
        "logs/MAGs/backmapping/depth/{sam_name}B_{sam_name2}_depth.benchmark"
    shell:
        "samtools view -bh {input.sam} | samtools sort -T {params.tmp} - > {output.bam}; "
        "export OMP_NUM_THREADS={threads}; "
        "jgi_summarize_bam_contig_depths --outputDepth {output.depth} {output.bam}"

rule merge_depths:
    input:
        depth_files = expand("../scratch_link/MAG_binning/backmapping/{sam_name}B/{sam_name}B_mapped_to_{{sam_name2}}B_contigs_parsed.depth", sam_name=config["sam_names"])
    output:
        depth_file_merged = "../results/MAG_binning/backmapping/merged_depths/{sam_name2}B_global_depth.txt"
    resources:
        account="pengel_beemicrophage",
        runtime="1:00:00",
        mem_mb = 10000
    threads: 4
    conda: "envs/mags_env.yaml"
    log:
        "logs/MAGs/backmapping/depth/{sam_name2}B_merge_depth.log"
    shell:
        "scripts/MAGs/merge_depths.pl {input.depth_files} > {output.depth_file_merged}"

##################################### (3) ######################################
# The following rule is used to bin the contigs of every assembly into MAGs
# using the mapping information of section 2

rule binning:
    input:
        assembly = "../results/assembly/HF_assembly/metaspades/{sam_name2}B_metaspades/{sam_name2}B_contigs_parsed.fasta",
        depth_file_merged = "../results/MAG_binning/backmapping/merged_depths/{sam_name2}B_global_depth.txt"
    output:
        dir=directory("../results/MAG_binning/bins/{sam_name2}B-metabat2/")
    params:
        min_contig_size=2500, # Metabat2 default
        min_bin_size=200000, # Metabat2 default
        max_edges=200, # Metabat2 default
        min_cv=1, # Metabat2 default
        marker = "../results/MAG_binning/bins/{sam_name2}B-metabat2.bins.done"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 2000,
        runtime= "01:00:00"
    threads: 16
    conda: "envs/mags_env.yaml"
    log: "logs/MAGs/binning/{sam_name2}B_binning.log"
    benchmark: "logs/MAGs/binning/{sam_name2}B_binning.benchmark"
    shell:
        "marker={params.marker}; "
        "prefix=${{marker/\.bins*/}}/{wildcards.sam_name2}B_MAG; "
        "mkdir -p {output.dir}; "
        "metabat2 -i {input.assembly} -a {input.depth_file_merged} -o ${{prefix}} --minContig {params.min_contig_size} --maxEdges {params.max_edges} -x {params.min_cv} --numThreads {threads}"


##################################### (4) ######################################
# These rules rule perform QC of the the MAGS

# Run checkm on all MAGs
rule checkm_QC:
    input:
        dir="../results/MAG_binning/bins/{sam_name2}B-metabat2/",
    output:
        dir=temp(directory("../results/MAG_binning/checkm_QC/{sam_name2}B_checkm_QC/")),
        file="../results/MAG_binning/checkm_QC/{sam_name2}B_checkm_QC/{sam_name2}B_checkm_QC_stats.tsv"
    log:
        "logs/MAGs/checkm/{sam_name2}B_checkm_QC.log"
    benchmark:
        "logs/MAGs/checkm/{sam_name2}B_checkm_QC.benchmark"
    threads: 16
    params:
        db="resources/default_DBs/checkm_db"
    conda:
        "envs/checkm_env.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "01:00:00"
    shell:
    #   "checkm data SetRoot | echo {params.db}" #TODO maybe this works
        "export CHECKM_DATA_PATH={params.db}; " #TODO there is a problem in setting the database with check data SetRoot. I had to enter the conda environment and set it myself, apparently this doesn't happen in v1.2.1
        "checkm lineage_wf {input.dir} {output.dir} -x fa -t {threads}; "
        "checkm qa {output.dir}/lineage.ms {output.dir} -o 2 -f {output.file} --tab_table"

# Find MAGs with >50% completeness and <10% contamination and aggreagate them
rule aggregate_checkm_QC:
    input:
        stats=expand("../results/MAG_binning/checkm_QC/{sam_name2}B_checkm_QC/{sam_name2}B_checkm_QC_stats.tsv", sam_name2=config["sam_names"])
    output:
        full_stats="../results/MAG_binning/checkm_QC/summary/all_MAGs_stats.tsv",
        filtered_stats="../results/MAG_binning/checkm_QC/summary/filtered_MAGs_stats.tsv",
        filered_mags=directory("../results/MAG_binning/bins/filtered_mags/")
    log:
        "logs/MAGs/checkm/aggregate_checkm.log"
    threads: 1
    params:
        bins="../results/MAG_binning/bins", #TODO this can be passsed as expanded input of the rule before
        compl=75,
        cont=10
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 10000,
        runtime= "00:30:00"
    script:
        "scripts/MAGs/aggregate_checkm.py"

# Taxonomically classify the filtered MAGs
rule classify_gtdbtk:
    input:
        filtered_mags="../results/MAG_binning/bins/filtered_mags/"
    output:
        class_out=directory("../results/MAG_binning/gtdbtk_classification/")
    log:
        "logs/MAGs/gtdbtk/gtdbtk_classification.log"
    benchmark:
        "logs/MAGs/gtdbtk/gtdbtk_classification.benchmark"
    threads: 8
    conda:
        "envs/gtdbk_env.yaml"
    params:
        db="resources/default_DBs/gtdbtk-2.1.1/db"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 150000,
        runtime= "03:00:00"
    shell:
        "export GTDBTK_DATA_PATH={params.db}; "
        "gtdbtk classify_wf --genome_dir {input.filtered_mags} --extension fa --out_dir {output.class_out} --cpus {threads}"

rule classify_gtdbtk_all:
    input:
        filtered_mags="../scratch_link/reference_genomes_redundant"
    output:
        class_out=directory("../results/MAG_binning/gtdbtk_classification_allGenomesTree/")
    log:
        "logs/MAGs/gtdbtk/gtdbtk_classification_all.log"
    benchmark:
        "logs/MAGs/gtdbtk/gtdbtk_classification_all.benchmark"
    threads: 25
    conda:
        "envs/gtdbk_env.yaml"
    params:
        db="resources/default_DBs/gtdbtk-2.1.1/db"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 150000,
        runtime= "10:00:00"
    shell:
        "export GTDBTK_DATA_PATH={params.db}; "
        "gtdbtk de_novo_wf --genome_dir {input.filtered_mags} --bacteria --outgroup_taxon p__Patescibacteria --force --extension fa --out_dir {output.class_out} --cpus {threads}"


################################################################################
########################## Bacterial Reference Database ########################
################################################################################
# Once we get all our good quality MAGs, it is time to create a reference Database.
# To do so, all filtered MAGs will be aggreagted in a directory with 211 genomes
# from isolates of A. mellifera and A. cerana gut microbiota. Then, Genomes will
# be dereplicated at 95% ANI using dRep. This will yield a species-level database,
# where every entry should correspond to a species.
################################################################################
################################################################################

# This rule aggregates the filtered MAGs and reference genomes in one directory
rule aggregate_refs:
    input:
        filtered_mags="../results/MAG_binning/bins/filtered_mags/",
        refs_isolates="../data/reference_assemblies/hb_bacteria/non_redundant/single_genomes/contigs/"
    output:
        all_refs=directory("../scratch_link/reference_genomes_redundant/")
    log:
        "logs/ref_db/aggregate.log"
    threads: 2
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 1500,
        runtime= "00:30:00"
    shell:
        "mkdir -p {output}; "
        "cp {input.filtered_mags}/*.fa {output}; "
        "cp {input.refs_isolates}/*.fna {output}"

# This rule runs dRep for 95% dereplication
rule run_dRep:
    input:
        filtered_mags="../results/MAG_binning/bins/filtered_mags/",
        refs_isolates="../data/reference_assemblies/hb_bacteria/non_redundant/single_genomes/contigs/"
    output:
        tmp_all=temp(directory("../scratch_link/reference_genomes_redundant_fordRep/")),
        drep_out=directory("../results/reference_db/")
    log:"logs/ref_db/dRep.log"
    benchmark:"logs/ref_db/dRep.benchmark"
    threads: 25
    conda:
        "envs/drep_env.yaml"
    params:
        compl=75
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "04:00:00"
    shell:
        "mkdir -p {output.tmp_all}; "
        "cp {input.filtered_mags}/*.fa {output.tmp_all}; "
        "cp {input.refs_isolates}/*.fna {output.tmp_all}; "
        "dRep dereplicate {output.drep_out} -g {output.tmp_all}/* --clusterAlg single --completeness {params.compl} -p {threads}"

# This rule aggreagtes dRep data with checkm and gtdb-TK data to obtain some summary tables
checkpoint parse_dRep:
    input:
        checkm_filt="../results/MAG_binning/checkm_QC/summary/filtered_MAGs_stats.tsv",
        gtdb_dir="../results/MAG_binning/gtdbtk_classification/",
        clust_dir="../results/reference_db",
        mtdata= "../data/metadata/RefGenomes_isolates_mtdata.csv"
    output:
        clust_info="../results/reference_db_filtered/summary_data_tables/clust_info.tsv",
        clust_assign="../results/reference_db_filtered/summary_data_tables/clust_assign.tsv",
        to_delete="../results/reference_db_filtered/summary_data_tables/to_delete.tsv",
        clust_final="../results/reference_db_filtered/summary_data_tables/clust_filtered.tsv",
        clust_win_final="../results/reference_db_filtered/summary_data_tables/clust_filtered_winners.tsv"
    conda:
        "envs/base_R_env.yaml"
    log:
        "logs/ref_db/parse_drep.log"
    params:
        us_func="scripts/useful_func.R"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 5000,
        runtime= "00:30:00"
    script:
        "scripts/MAGs/parse_drep.R"

# This rule plots dendograms with taxonomic info for each cluster, also it deletes fro, the dreplicated database all cluster that have no classification at the genus level
rule filter_dRep_db:
    input:
        clust_assign="../results/reference_db_filtered/summary_data_tables/clust_assign.tsv",
        mtdata= "../data/metadata/RefGenomes_isolates_mtdata.csv",
        to_delete="../results/reference_db_filtered/summary_data_tables/to_delete.tsv",
        ref_db="../results/reference_db/"
    output:
        out_fgs=directory("../results/reference_db_filtered/figures/secondary_clusters_dendograms/"),
        filt_db=directory("../results/reference_db_filtered/dereplicated_genomes_filtered/")
    conda:
        "envs/drep_env.yaml"
    log:
        "logs/ref_db/filter_drep_db.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 10000,
        runtime= "00:30:00"
    script:
        "scripts/MAGs/analyze_drep.py"

################################################################################
############################## Phage identification  ###########################
################################################################################
#This part of the pipeline runs four tools on all the assembly, to identify
#phages, these sequences will then dereplicated and binned. Each phages id tools
#ouput are parsed and merged, a confidence score is attributed to each id
#and a fasta file of the phages sequences is extracted :
#   (1): PHAGES id tools
#   (1.1) concat assembly
#   (1.2) run phage id tools (virsorter2, viralverify, deepvirfinder, vibrant)
#   (2) : Parsing outputs
#   (3): Run ChecV
################################################################################

##################################### (1.1) ######################################

rule concat_assembly:
    input:
        assemblies= expand("../results/assembly/HF_assembly/{assembler}/{{sample}}_{assembler}/{{sample}}_contigs_parsed.fasta", assembler=assemblers),
    output:
        concat_assembly = "../results/assembly/concat_assembly/{sample}_concat_assembly.fasta"
    log:
        "logs/assembly/concat_assembly/{sample}_concat.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 1000,
        runtime = "00:15:00"
    threads:1
    shell:
        "cat  {input} > {output.concat_assembly}"

##################################### (1.2) ######################################

rule run_virsorter:
    input:
        "../results/assembly/concat_assembly/{sample}_concat_assembly.fasta"
    output:
        outdir = temp(directory("../scratch_link/viral_identification/virsorter/{sample}_virsorter/")),
        score = "../results/viral_identification/virsorter/{sample}_virsorter/{sample}_virsorter_score.tsv",
        prophages="../results/viral_identification/virsorter/{sample}_virsorter/{sample}_virsorter_boundaries.tsv"
    params:
        db = directory("resources/default_DBs/virsorter_db"),
        container="resources/containers/virsorter2.sif"
    log:
        "logs/viral_identification/virsorter/{sample}_virsorter.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime = "04:00:00"
    threads: 20
    conda:
        "envs/singularity.yaml"
    shell:
        "org_dir=$PWD; "
        "infile=$(basename {input}); "
        "mkdir -p {output.outdir}; "
        "cp {params.container} {output.outdir}; cp {input} {output.outdir}; "
        "cd {output.outdir}; "
        "singularity run -B $PWD virsorter2.sif run -w ./output -i ${{infile}} --keep-original-seq -j {threads};"
        "rm virsorter2.sif ${{infile}}; "
        "cd ${{org_dir}}; "
        "dir=$(dirname {output.score}); mkdir -p ${{dir}}; "
        "mv {output.outdir}/output/final-viral-score.tsv {output.score}; "
        "mv {output.outdir}/output/final-viral-boundary.tsv {output.prophages}"

rule run_viralverify:
    input:
        "../results/assembly/concat_assembly/{sample}_concat_assembly.fasta"
    output:
        outdir = temp(directory("../scratch_link/viral_identification/viralverify/{sample}_viralverify/")),
        score = "../results/viral_identification/viralverify/{sample}_viralverify/{sample}_viralverify_score.csv"
    params:
        hmm = "resources/default_DBs/nbc_hmms.hmm"
    log:
        "logs/viral_identification/viralverify/{sample}_viralverify.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime = "04:0:00"
    threads:20
    conda:
        "envs/viralverify.yaml"
    shell:
        "viralverify -f {input} -o {output.outdir} --hmm {params.hmm} -t {threads};"
        "dir=$(dirname {output.score}); mkdir -p ${{dir}}; "
        "mv {output.outdir}/{wildcards.sample}_concat_assembly_result_table.csv {output.score}"

rule run_vibrant:
    input:
        assembly = "../results/assembly/concat_assembly/{sample}_concat_assembly.fasta"
    output:
        outdir = temp(directory("../scratch_link/viral_identification/vibrant/{sample}/")),
        score = "../results/viral_identification/vibrant/{sample}_vibrant/{sample}_vibrant_score.tsv",
        prophages="../results/viral_identification/vibrant/{sample}_vibrant/{sample}_vibrant_prophages.tsv"
    log:
        "logs/viral_identification/vibrant/{sample}_vibrant.log"
    params:
        db="resources/default_DBs/databases"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime = "04:00:00"
    threads:20
    conda:
        "envs/vibrant.yaml"
    shell:
        "VIBRANT_run.py -i {input.assembly} -folder {output.outdir} -d {params.db} -t {threads};"
        "dir=$(dirname {output.score}); mkdir -p ${{dir}}; "
        "mv {output.outdir}/VIBRANT_{wildcards.sample}_concat_assembly/VIBRANT_results_{wildcards.sample}_concat_assembly/VIBRANT_genome_quality_{wildcards.sample}_concat_assembly.tsv {output.score}; "
        "mv {output.outdir}/VIBRANT_{wildcards.sample}_concat_assembly/VIBRANT_results_{wildcards.sample}_concat_assembly/VIBRANT_integrated_prophage_coordinates_{wildcards.sample}_concat_assembly.tsv {output.prophages};"
        "cp {output.outdir}/VIBRANT_{wildcards.sample}_concat_assembly/VIBRANT_results_{wildcards.sample}_concat_assembly/* ${{dir}}"

##################################### (2) ######################################
rule parse_VI:
    input:
        vib_score = expand("../results/viral_identification/vibrant/{sample}_vibrant/{sample}_vibrant_score.tsv", sample=config["samples"]),
        vib_prophages=expand("../results/viral_identification/vibrant/{sample}_vibrant/{sample}_vibrant_prophages.tsv", sample=config["samples"]),
        vv_score = expand("../results/viral_identification/viralverify/{sample}_viralverify/{sample}_viralverify_score.csv", sample=config["samples"]),
        vs_score = expand("../results/viral_identification/virsorter/{sample}_virsorter/{sample}_virsorter_score.tsv", sample=config["samples"]),
        vs_prophages= expand("../results/viral_identification/virsorter/{sample}_virsorter/{sample}_virsorter_boundaries.tsv", sample=config["samples"])
    output:
        all_scores ="../results/viral_identification/ViralIdentification_scores.tsv",
        vib_scores="../results/viral_identification/vibrant/vibrant_all_scores.tsv",
        vv_scores ="../results/viral_identification/viralverify/viralverify_all_scores.tsv",
        vs_scores ="../results/viral_identification/virsorter/virsorter_all_scores.tsv",
        tab="../results/viral_identification/identification_tools_tab.tsv"
    log:
        "logs/viral_identification/parse_VI.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 10000,
        runtime = "00:10:00"
    threads:1
    conda:
        "envs/base_R_env.yaml"
    script:
        "scripts/viral_identification/parse_viral_identification.R"

rule extract_viral_contigs:
    input:
        all_scores ="../results/viral_identification/ViralIdentification_scores.tsv",
        concat_assembly = "../results/assembly/concat_assembly/{sample}_concat_assembly.fasta"
    output:
        "../results/assembly/viral_contigs/single_sample/{sample}_viral_contigs.fasta"
    log:
        "logs/viral_identification/extract_viruses/{sample}_viral_contigs.log"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 10000,
        runtime = "00:20:00"
    threads:1
    conda:
        "envs/mOTUpan.yaml"
    shell:
        "python scripts/viral_identification/extract_viruses_from_assembly.py {input.concat_assembly} {input.all_scores} {output}"

##################################### (3) ######################################
# rule run_checkv:
#     input:
#         assembly = "../results/assembly/viral_contigs/single_sample/{sample}_viral_contigs.fasta"
#     output:
#         trimmed="../results/assembly/viral_contigs/single_sample/{sample}_viral_contigs_trimmed.fasta",
#         tab="../results/viral_identification/checkv/{sample}_checkv_quality_summary.tsv"
#     log:
#         "logs/viral_identification/checkv/{sample}_checkv.log"
#     params:
#         outdir = directory("../scratch_link/viral_identification/checkv/{sample}/"),
#         db="resources/default_DBs/checkv-db-v1.5/"
#     resources:
#         account="pengel_beemicrophage",
#         mem_mb= 100000,
#         runtime = "04:00:00"
#     threads:20
#     conda:
#         "envs/checkv.yaml"
#     shell:
#         "mkdir -p {params.outdir}; "
#         "checkv end_to_end {input.assembly} {params.outdir} -t {threads} -d {params.db}; "
#         "mv {params.outdir}/quality_summary.tsv {output.tab}; "
#         "cat {params.outdir}/viruses.fna {params.outdir}/proviruses.fna > {output.trimmed}"


####################################################################################
################################## viralMAGS #######################################
####################################################################################
# This part of the pipleine takes takes host contigs identified as viral and
# maps bins them into vMAGS using vRhyme.
# The procedure is similar to the one to create MAGs, just using vRhyme as binning tool.
################################################################################
################################################################################

# This rule builds a mapping index for each sample
rule build_backmapping_index_viruses:
    input:
        ref="../results/assembly/viral_contigs/single_sample/{asmbl}_viral_contigs.fasta"
    output:
        dir=directory("../results/assembly/viral_contigs/indexes/{asmbl}_viral_contigs/{asmbl}_viral_contigs_index")
    conda:
        "envs/map_env.yaml"
    threads: 15
    params:
        basename="{asmbl}_viral_contigs_trimmed"
    log:
        "logs/vMAGs/backmapping/indexing/{asmbl}_build_bowtie_index.log"
    resources:
        account= "pengel_beemicrophage",
        mem_mb= 20000,
        runtime= "01:00:00"
    shell:
        "mkdir -p {output.dir}; "
        "bowtie2-build {input.ref} {output.dir}/{params.basename} --threads {threads}"

# This rule uses bowtie2 to map reads to the viral contigs
rule backmapping_viral:
    input:
        assembly = "../results/assembly/viral_contigs/indexes/{asmbl}_viral_contigs/{asmbl}_viral_contigs_index", # we bin only bacteria
        R1 = "../data/host_filtered_reads/{sample}_R1_HF.fastq.gz",
        R2 = "../data/host_filtered_reads/{sample}_R2_HF.fastq.gz"
    output:
        sam=temp("../scratch_link/vMAG_binning/backmapping/{sample}/{sample}_mapped_to_{asmbl}_viral_contigs.sam")
    resources:
        account="pengel_beemicrophage",
        runtime="10:00:00",
        mem_mb = 10000
    params:
        basename="{asmbl}_viral_contigs_trimmed"
    threads: 15
    log:
        "../scratch_link/logs/{sample}_to_{asmbl}_back.log"
    conda: 
        "envs/map_env.yaml"
    shell:
        "bowtie2 -x {input.assembly}/{params.basename} -1 {input.R1} -2 {input.R2} -S {output.sam} --threads {threads}"

# This rule takes the bam file and creates a depth file
rule backmapping_depths_viral:
    input:
        sam= "../scratch_link/vMAG_binning/backmapping/{sample}/{sample}_mapped_to_{asmbl}_viral_contigs.sam"
    output:
        bam= temp("../scratch_link/vMAG_binning/backmapping/{sample}/{sample}_mapped_to_{asmbl}_viral_contigs.bam"),
        depth= temp("../scratch_link/vMAG_binning/backmapping/{sample}/{sample}_mapped_to_{asmbl}_viral_contigs.depth")
    resources:
        account="pengel_beemicrophage",
        runtime="01:30:00",
        mem_mb = 10000
    params:
        tmp="../scratch_link/tmp"
    threads: 15
    conda: 
        "envs/sam_env.yaml"
    log:
        "../scratch_link/logs/{sample}_to_{asmbl}_back_depth.log"
    shell:
        "samtools view -bh {input.sam} -@ {threads} | samtools sort -T {params.tmp} - > {output.bam}; "
        "export OMP_NUM_THREADS={threads}; "
        "jgi_summarize_bam_contig_depths --outputDepth {output.depth} {output.bam}"

# This rule merges the depth files
rule merge_depths_viral: 
    input: 
        depth_files = expand("../scratch_link/vMAG_binning/backmapping/{sample}/{sample}_mapped_to_{{asmbl}}_viral_contigs.depth", sample=config["samples"])
    output:
        depth_file_merged="../results/vMAG_binning/backmapping/merged_depths/{asmbl}_global_viral_depth.txt"
    resources:
        account="pengel_beemicrophage",
        runtime="1:00:00",
        mem_mb = 10000
    threads: 1
    conda: 
        "envs/mags_env.yaml"
    log:
        "logs/vMAGs/backmapping/depth/{asmbl}_merge_depth.log"
    shell:
        "scripts/MAGs/merge_depths.pl {input.depth_files} > {output.depth_file_merged}"

# this rules convert depth files from METAbat2 format to vRhyme format
rule covert_depths:
    input:
        depth_merged = "../results/vMAG_binning/backmapping/merged_depths/{asmbl}_global_viral_depth.txt"
    output:
        depth_file_merged_vrhyme = "../results/vMAG_binning/backmapping/merged_depths/{asmbl}_global_depth_vrhyme.txt"
    resources:
        account="pengel_beemicrophage",
        runtime="1:00:00",
        mem_mb = 10000
    threads: 1
    params:
        conda="resources/conda_envs/vrhyme"
    log:
        "logs/vMAGs/backmapping/depth/convert/{asmbl}_covert_depth.log"
    shell:
        """
        bash -c '. $HOME/.bashrc
            conda activate {params.conda}
            coverage_table_convert.py -i {input} -o {output}'
        """

# this rule runs vRhyme
rule run_vRhyme:
    input:
        assembly="../results/assembly/viral_contigs/single_sample/{asmbl}_viral_contigs.fasta",
        depth_file = "../results/vMAG_binning/backmapping/merged_depths/{asmbl}_global_depth_vrhyme.txt"
    output:
        out= directory("../results/vMAG_binning/vRhyme_bins/{asmbl}/")
    resources:
        account="pengel_beemicrophage",
        runtime="0:30:00",
        mem_mb = 100000
    threads: 15
    params:
        conda="resources/conda_envs/vrhyme",
        tmp="../scratch_link/vMAG_binning/vRhyme_bins/{asmbl}"
    log:
        "logs/vMAGs/backmapping/binning/{asmbl}_virus_binning.log"
    benchmark:
        "logs/vMAGs/backmapping/binning/{asmbl}_virus_binning.benchmark"
    shell:
        """
        bash -c '. $HOME/.bashrc
            conda activate {params.conda}
            vRhyme -i {input.assembly} -c {input.depth_file} -o {params.tmp} --prefix {wildcards.asmbl}_vMAG_ --iter 11 --method longest --derep_id 1  -t {threads}'
            mkdir -p {output}
            mv {params.tmp}/* {output}/
            rm -rf {output}/vRhyme_alternate_bins
        """

####################################################################################
################################## POLISH VIRAL CONTIGS ############################
####################################################################################
# The foilowing rules will:

# 1. remove bacterial contigs (size > 500kb and bacteria according to checkM)
# 2. dereplicate for identical contigs across samples
# 3. run CheckV again to have a last table with all the phages
################################################################################
################################################################################

# this rule bins the viral contigs using the bins from vRhyme
rule bin_viral_contigs:
    input:
        assembly = "../results/assembly/viral_contigs/single_sample/{sample}_viral_contigs.fasta",
        bins="../results/vMAG_binning/vRhyme_bins/{sample}"
    output:
        binned_fasta="../results/assembly/viral_contigs/binning/{sample}_viral_contigs_binned.fasta",
        binning_data="../results/vMAG_binning/summary_tables/{sample}_binning_data.tsv"
    resources:
        account="pengel_beemicrophage",
        runtime="1:00:00",
        mem_mb = 100000
    threads: 1
    conda:
        "envs/mOTUpan.yaml"
    log:
        "logs/vMAGs/vRhyme/{sample}_binning_fasta.log"
    shell:
        "python scripts/vMAGs/bin_contigs.py -i {input.assembly}  -d {input.bins} -f {output.binned_fasta} -t {output.binning_data}"

# this rule runs checkV on the binned viral contigs
rule run_checkv:
    input:
        assembly = "../results/assembly/viral_contigs/binning/{sample}_viral_contigs_binned.fasta"
    output:
        dir=temp(directory("../results/vMAG_binning/polishing/checkv_viral_contigs/{sample}_checkv")),
        trimmed="../results/assembly/viral_contigs/trimmed/{sample}_viral_contigs_trimmed.fasta",
        tab="../results/vMAG_binning/polishing/checkv/{sample}_checkv_quality_summary.tsv"
    params:
        db="resources/default_DBs/checkv-db-v1.5/"
    resources:
        account="pengel_beemicrophage",
        mem_mb= 100000,
        runtime = "02:00:00"
    threads:20
    conda:
        "envs/checkv.yaml"
    log:
        "logs/polish_vMAGs/checkV/{sample}_checkV.log"
    benchmark:
        "logs/polish_vMAGs/checkV/{sample}_checkV.benchmark"
    shell:
        "checkv end_to_end {input.assembly} {output.dir} -t {threads} -d {params.db}; "
        "mv {output.dir}/quality_summary.tsv {output.tab}; "
        "cat {output.dir}/viruses.fna {output.dir}/proviruses.fna > {output.trimmed}"

# this rule filters out contigs that are not viral (found with low confidence by only one tool, too long or too short)
rule viral_contigs_quality_filtering:
    input:
        assembly="../results/assembly/viral_contigs/trimmed/{sample}_viral_contigs_trimmed.fasta",
        binning_data="../results/vMAG_binning/summary_tables/{sample}_binning_data.tsv",
        vi="../results/viral_identification/ViralIdentification_scores.tsv",
        checkv="../results/vMAG_binning/polishing/checkv/{sample}_checkv_quality_summary.tsv"
    output:
        filtered_fasta="../results/assembly/viral_contigs/QC_filtered/{sample}_viral_contigs_filtered.fasta",
        filtering_data=temp("../results/vMAG_binning/summary_tables/{sample}_viral_contigs_metadata.tsv")
    resources:
        account="pengel_beemicrophage",
        runtime="1:00:00",
        mem_mb = 100000
    threads: 1
    conda:
        "envs/mOTUpan.yaml"
    log:
        "logs/vMAGs/polishing/{sample}_filtering_vMAGs.log"
    benchmark:
        "logs/vMAGs/vRhyme/{sample}_filtering_vMAGs.benchmark"
    shell:
        "python scripts/vMAGs/parse_vMAGs_binning.py -i {input.assembly} -b {input.binning_data} -v {input.vi} -c {input.checkv} -f {output.filtered_fasta}  -t {output.filtering_data}" # TODO contigs that are trimmed like 10B_vBin_6_1 6260-11927/11927 do not end up in the final filtered file
# TODO other example 69B_vBin_6_1 1-67153/288529 and 69B_vBin_6_2 161403-209164/288529. at this stage contigs that are not binned are still named after the original contigs. I should rename them to the bin name

rule split_contigs_checkm:
    input:
        "../results/assembly/viral_contigs/QC_filtered/{sample}_viral_contigs_filtered.fasta",
    output:
        tmp_dir=temp(directory("../results/vMAG_binning/polishing/{sample}_single_genomes")),
    resources:
        account="pengel_beemicrophage",
        runtime="0:30:00",
        mem_mb = 8000
    threads: 1
    conda:
        "envs/mOTUpan.yaml"
    log:
        "logs/polish_vMAGs/checkm/{sample}_split_for_checkm.log"
    shell:
        "python scripts/assembly/split_assembly.py -f {input} -d {output}"

rule checkm_viral_contigs:
    input:
        dir="../results/vMAG_binning/polishing/{sample}_single_genomes"
    output:
        dir=directory("../results/vMAG_binning/polishing/checkm/{sample}_checkm"),
        file="../results/vMAG_binning/polishing/checkm/{sample}_checkm_stats.tsv"
    log:
        "logs/polish_vMAGs/checkm/{sample}_checkm.log"
    benchmark:
        "logs/polish_vMAGs/checkm/{sample}_checkm.benchmark"
    threads: 25
    params:
        db="resources/default_DBs/checkm_db"
    conda:
        "envs/checkm_env.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 200000,
        runtime= "04:00:00"
    shell:
        "export CHECKM_DATA_PATH={params.db}; "
        "checkm lineage_wf {input} {output.dir} -x fasta -t {threads}; "
        "checkm qa {output.dir}/lineage.ms {output.dir} -o 2 -f {output.file} --tab_table"

rule filter_bacteria_from_viral_contigs:
    input:
        fasta="../results/assembly/viral_contigs/QC_filtered/{sample}_viral_contigs_filtered.fasta",
        checkm="../results/vMAG_binning/polishing/checkm/{sample}_checkm_stats.tsv",
        mtdata="../results/vMAG_binning/summary_tables/{sample}_viral_contigs_metadata.tsv"
    output:
        fasta_nobact="../results/assembly/viral_contigs/polished/{sample}_viral_contigs_filtered_nobact.fasta",
        mtdata_nobact="../results/vMAG_binning/summary_tables/{sample}_viral_contigs_metadata_nobact.tsv"
    resources:
        account="pengel_beemicrophage",
        runtime="0:30:00",
        mem_mb = 8000
    threads: 1
    conda:
        "envs/mOTUpan.yaml"
    log:
        "logs/polish_vMAGs/checkm/{sample}_remove_bacteria.log"
    shell:
        "python scripts/vMAGs/filter_bacteria_from_viral_contigs.py -f {input.fasta} -c {input.checkm} -m {input.mtdata} -o {output.fasta_nobact} -t {output.mtdata_nobact}"

rule dereplicate_viral_contigs:
    input:
        fasta=expand("../results/assembly/viral_contigs/polished/{sample}_viral_contigs_filtered_nobact.fasta", sample=config["samples"]),
    output:
        all_fasta="../results/assembly/viral_contigs/dereplicated/all_viral_contigs.fasta",
        concat_cont_drep="../results/assembly/viral_contigs/dereplicated/all_viral_contigs_drep.fasta",
        concat_cont_clst=temp("../results/assembly/viral_contigs/dereplicated/all_viral_contigs_drep.fasta.clstr")
    resources:
        account="pengel_beemicrophage",
        runtime="04:00:00",
        mem_mb = 200000
    threads: 25
    log:
        "logs/polish_vMAGs/dereplication/drep.log"
    benchmark:
        "logs/polish_vMAGs/dereplication/drep.benchmark"
    conda:
        "envs/cd-hit.yaml"
    shell:
        "cat {input.fasta} > {output.all_fasta}; "
        "cd-hit-est -i {output.all_fasta} -o {output.concat_cont_drep} -c 0.95 -aS 0.85 -M 0 -d 0 -T {threads}"

rule align_viral_contigs:
    input:
        fasta="../results/assembly/viral_contigs/dereplicated/all_viral_contigs.fasta",
    output:
        directory("../results/assembly/viral_contigs/dereplicated/all_genomes_alignment")
    resources:
        account="pengel_beemicrophage",
        runtime="01:00:00",
        mem_mb = 200000
    threads: 16
    log:
        "logs/polish_vMAGs/dereplication/align_viral_genomes.log"
    benchmark:
        "logs/polish_vMAGs/dereplication/align_viral_genomes.benchmark"
    conda:
        "envs/drep_env.yaml"
    shell:
        "python scripts/vMAGs/align_viral_contigs.py -i {input} -o {output} -t {threads}"

rule parse_drep_viral_contigs:
    input:
        derep_fasta="../results/assembly/viral_contigs/dereplicated/all_viral_contigs_drep.fasta.clstr"
    output:
        "../results/vMAG_binning/summary_tables/all_viral_contigs_dereplication.tsv"
    resources:
        account="pengel_beemicrophage",
        runtime="01:00:00",
        mem_mb = 20000
    conda:
        "envs/mOTUpan.yaml"
    log:
        "logs/polish_vMAGs/dereplication_parsing.log"
    benchmark:
        "logs/polish_vMAGs/dereplication_parsing.benchmark"
    shell:
        "python scripts/vMAGs/parse_cdhit.py -i {input} -o {output}" 

rule aggreagte_polishing_tables:
    input:
        checkv= expand("../results/vMAG_binning/polishing/checkv/{sample}_checkv_quality_summary.tsv", sample=config["samples"]),
        checkm= expand("../results/vMAG_binning/polishing/checkm/{sample}_checkm_stats.tsv", sample=config["samples"]),
        mtdata_nobact=expand("../results/vMAG_binning/summary_tables/{sample}_viral_contigs_metadata_nobact.tsv", sample=config["samples"]),
        drep="../results/vMAG_binning/summary_tables/all_viral_contigs_dereplication.tsv"
    output:
        all_checkv="../results/vMAG_binning/polishing/checkv/all_checkv_quality_summary.tsv",
        all_checkm="../results/vMAG_binning/polishing/checkm/all_checkm_stats.tsv",
        all_mtdata_nobact="../results/vMAG_binning/summary_tables/all_viral_contigs_metadata.tsv"
    resources:
        account="pengel_beemicrophage",
        runtime="01:00:00",
        mem_mb = 20000
    log:
        "logs/polish_vMAGs/aggregrate_polishing_tables.log"
    benchmark:
        "logs/polish_vMAGs/aggregrate_polishing_tables.benchmark"
    run:
        import pandas as pd
        # open drep
        drep=pd.read_csv(input.drep, sep="\t")
        # keep only rows where reference=True
        drep=drep[drep["reference"]==True]
        # store contig_id in a list
        drep_contigs=drep["contig_id"].tolist()
        checkV=concat_tables(input.checkv)
        checkM=concat_tables(input.checkm)
        mtdata_nobact=concat_tables(input.mtdata_nobact)
        checkV.to_csv(output.all_checkv, sep="\t", index=False)
        checkM.to_csv(output.all_checkm, sep="\t", index=False)
        # add column "reference" to mtdata_nobact
        mtdata_nobact["reference"]=mtdata_nobact["genome_id"].isin(drep_contigs)
        mtdata_nobact.to_csv(output.all_mtdata_nobact, sep="\t", index=False)


####################################################################################
################################## vContact2 #######################################
####################################################################################
# The foilowing rules are used to prepare the viral contigs/bins file to run vContact2
################################################################################
################################################################################

# this rule split the contigs into single genomes fasta files
rule split_contig:
    input:
        filt=expand("../results/assembly/viral_contigs/polished/{sample}_viral_contigs_filtered_nobact.fasta", sample=config["samples"])
    output:
        filt_dir=temp(directory("../scratch_link/viral_contigs/single_genomes/"))
    resources:
        account="pengel_beemicrophage",
        runtime="0:30:00",
        mem_mb = 8000
    threads: 1
    conda:
        "envs/mOTUpan.yaml"
    log:
        "logs/vMAGs/split_contigs.log"
    shell:
        """
        for file in {input.filt} ; do
            python scripts/assembly/split_assembly.py -f $file -d {output.filt_dir}
        done
        """

rule prodigal_concat_bins:
    input:
        agg_fasta="../results/assembly/viral_contigs/dereplicated/all_viral_contigs_drep.fasta",
    output:
        proteins = "../results/assembly/viral_contigs/annotations/all_viral_contigs_drep.faa",
        genes = "../results/assembly/viral_contigs/annotations/all_viral_contigs_drep.fna"
    threads: 1
    conda:
        "envs/prodigal.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 8000,
        runtime= "01:30:00"
    log:
        "logs/vMAGs/annotations/prodigal.log"
    shell:
        "prodigal -i {input.agg_fasta} -d {output.genes} -a {output.proteins} -p meta -g 11"

rule gene_2_genome:
    input:
        all_vprot = "../results/assembly/viral_contigs/annotations/all_viral_contigs_drep.faa"
    output:
        gene_2_genome = "../results/Vcontact2/gene_to_genome.csv"
    threads: 1
    conda:
        "envs/mags_env.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 8000,
        runtime= "00:10:00"
    log:
        "logs/vcontact/gene_to_genome.log"
    shell:
        "python scripts/Viral_classification/gene2genome.py -p {input.all_vprot} -o {output.gene_2_genome} -s 'Prodigal-FAA'"

rule run_vcontact:
    input:
        all_vprot = "../results/assembly/viral_contigs/annotations/all_viral_contigs_drep.faa",
        gene_2_genome = "../results/Vcontact2/gene_to_genome.csv"
    output:
        directory("../results/Vcontact2/vCONTACT_results")
    threads: 48
    params:
        condaenv="resources/conda_envs/vcontact2"
    log:
        "logs/vcontact/run_vcontact2.log"
    benchmark:
        "logs/vcontact/run_vcontact2.benchmark"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 512000,
        runtime= "15:00:00"
    shell:
        """
        bash -c '. $HOME/.bashrc
            conda activate {params.condaenv}
            vcontact2 -t {threads} --raw-proteins {input.all_vprot} --rel-mode 'Diamond' --proteins-fp {input.gene_2_genome} --db 'ProkaryoticViralRefSeq211-Merged' --pcs-mode MCL --vcs-mode ClusterONE --c1-bin {params.condaenv}/bin/cluster_one-1.0.jar --output-dir {output} -e 'cytoscape' -e 'csv''
        """

#################################################################################
############################## HOST ASSIGNATION #################################
#################################################################################
# to assign hosts to every viral contigs we will use the CRISPR spacers and genome homology:

# 1. find CRISPR spacers in all MAGs and refernce bacterial genome
# 2. Map spacers against all viral contigs
# 3. Use fastani to identify genome Homology between viral contigs and reference bacterial genomes
# 4. Use the results of 2 and 3 to assign a host to every viral contigs
#################################################################################

###################################### 1 ########################################
rule find_CRISPR_spacers:
    input:
        all_refs="../scratch_link/reference_genomes_redundant/"
    output:
        spacers_dir=temp("../results/spacers_db/hb_spacers/{gg}-spacers")
    log:
        "logs/spacers_db/find_spacers_{gg}.log"
    benchmark:
        "logs/spacers_db/find_spacers_{gg}.benchmark"
    threads: 1
    conda:
        "envs/singularity.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "04:00:00"
    shell:
        "scripts/assign_host/run_ccf.sh {input} {output} {wildcards.gg}"


rule parse_CCF:
    input:
        all_spacers=expand("../results/spacers_db/hb_spacers/{gg}-spacers", gg=get_genomes("../results/reference_db_filtered/summary_data_tables/clust_filtered.tsv"))
    output:
        parsed_crispr=directory("../results/spacers_db/hb_spacers_parsed/")
    log:
        "logs/spacers_db/parse_CFFF.log"
    benchmark:
        "logs/spacers_db/parse_CFFF.benchmark"
    threads: 1
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 10000,
        runtime= "00:30:00"
    script:
        "scripts/assign_host/CCF_Parser.py"

rule create_spacersDB:
    input:
        openDB_spacers="resources/default_DBs/CrisprOpenDB/CrisprOpenDB/SpacersDB/SpacersDB.fasta",
        my_spacers="../results/spacers_db/hb_spacers_parsed/"
    output:
        spacers_db=directory("../results/spacers_db/spacersDB/")
    log:
        "logs/spacers_db/create_spacersDB.log"
    benchmark:
        "logs/spacers_db/create_spacersDB.benchmark"
    threads: 1
    conda:
        "envs/blast.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 10000,
        runtime= "01:00:00"
    shell:
        "mkdir -p {output.spacers_db}; "
        "cat {input.my_spacers}/CRISPR_v1.fasta {input.openDB_spacers} > {output.spacers_db}/myspacersDB.fasta; "
        "makeblastdb -in {output.spacers_db}/myspacersDB.fasta -dbtype nucl -out {output.spacers_db}/mySpacersDB"

###################################### 2 ########################################
rule assign_host:
    input:
        phageDB="../results/assembly/viral_contigs/polished/{sample}_viral_contigs_filtered_nobact.fasta",
        spacers_db="../results/spacers_db/spacersDB/"
    output:
        blastout=temp("../results/host_assigniation/{sample}_host/spacers_{sample}_blastout.txt"),
        spacers_report=temp("../results/host_assigniation/{sample}_host/CRISPRopenDB_{sample}_report.txt")
    log:
        "logs/assign_host/assign_host_{sample}.log"
    benchmark:
        "logs/assign_host/assign_host_{sample}.benchmark"
    threads: 10
    conda:
        "envs/CrisprOpenDB.yaml"
    params:
        DB="resources/default_DBs/CrisprOpenDB"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "10:00:00"
    shell:
        "wd=$(pwd -P); "
        "scripts/assign_host/run_assign_host.sh ${{wd}} {input.phageDB} {input.spacers_db} {output.blastout} {output.spacers_report} {threads} {wildcards.sample}"


rule aggregate_assign_host:
    input:
        blastout=expand("../results/host_assigniation/{sample}_host/spacers_{sample}_blastout.txt", sample=config["samples"]),
        spacers_report=expand("../results/host_assigniation/{sample}_host/CRISPRopenDB_{sample}_report.txt", sample=config["samples"])
    output:
        all_blastout="../results/host_assigniation/spacers/all_spacers_blastout.txt",
        all_spacers_report="../results/host_assigniation/spacers/all_CRISPRopenDB_report.txt"
    log:
        "logs/assign_host/aggregate_assign_host.log"
    benchmark:
        "logs/assign_host/aggrgate_assign_host.benchmark"
    threads: 1
    conda:
        "envs/CrisprOpenDB.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "00:10:00"
    shell:
        "tail -n +2 -q {input.spacers_report}> {output.all_spacers_report}; "
        "echo -e 'Hit_nr,SPACER_ID,Query,identity,alignement_length,mismatch,gap,q_start,q_end,s_start,s_end,e_value,score,GENEBANK_ID,ORGANISM_NAME,SPECIES,GENUS,FAMILY,ORDER,SPACER,SPACER_LENGTH,COUNT_SPACER,POSITION_INSIDE_LOCUS,true_num_mismatch' > {output.all_blastout}; "
        "tail -n +2 -q {input.blastout} >> {output.all_blastout}; "

###################################### 3 ########################################
rule fastani_prophages:
    input:
        mags="../results/MAG_binning/bins/filtered_mags",
        isolates="../data/reference_assemblies/hb_bacteria/non_redundant/single_genomes/contigs",
        viruses="../scratch_link/viral_contigs/single_genomes/"
    output:
        "../results/host_assigniation/prophges/fastani_out.txt"
    threads: 15
    params:
        mags_l="./mags_l.txt",
        vir_l="./vir_l.txt"
    log:
        "logs/assign_host/prophages/fastani.log"
    conda:
        "envs/drep_env.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "02:00:00"
    shell:
        "find {input.mags} -maxdepth 1 -type f -not -name '.*' -printf '{input.mags}/%f\n' > {params.mags_l}; "
        "find {input.isolates} -maxdepth 1 -type f -not -name '.*' -printf '{input.isolates}/%f\n' >> {params.mags_l}; "
        "find {input.viruses} -maxdepth 1 -type f -not -name '.*' -printf '{input.viruses}/%f\n' > {params.vir_l}; "
        "fastANI --ql {params.vir_l} --rl {params.mags_l} -t {threads} --fragLen 3000 -o {output}; "
        "rm {params.mags_l} {params.vir_l}"

###################################### IPHOP ########################################
rule run_iphop:
    input:
        phage_fasta="../results/assembly/viral_contigs/polished/{sample}_viral_contigs_filtered_nobact.fasta",
    output:
        outdir=directory("../results/host_assigniation/IPHOP/{sample}_iphop")
    log:
        "logs/assign_host/iphop/assign_host_{sample}.log"
    benchmark:
        "logs/assign_host/iphop/assign_host_{sample}.benchmark"
    threads: 12
    params:
        DB="resources/db2/iphop_mine_rw/",
        old_DB="resources/db2/iphop/Sept_2021_pub_rw",
        container="resources/db2/containers/iphop-latest.simg"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 200000,
        runtime= "04:00:00"
    shell:
        """
        in=$(realpath {input.phage_fasta})
        out=$(realpath {output.outdir})
        db=$(realpath {params.DB})
        old_db=$(realpath {params.old_DB})

        mkdir -p $out
        
        singularity run -B $in,$db,$out,$old_db {params.container} predict --fa_file $in --db_dir $db --out_dir $out -t {threads}
        """
###################################### 4 ########################################
rule parse_host_assignation:
    input:
        all_blastout="../results/host_assigniation/spacers/all_spacers_blastout.txt",
        pro="../results/host_assigniation/prophges/fastani_out.txt",
        spacers_mtdata="../results/spacers_db/hb_spacers_parsed",
        clust_filtered="../results/reference_db_filtered/summary_data_tables/clust_filtered.tsv",
        binning_data="../results/vMAG_binning/summary_tables/all_viral_contigs_metadata.tsv"
    output:
       formatted_spacers="../results/host_assigniation/summary_table/spacers_metadata.tsv",
       blastout_filt_complete="../results/host_assigniation/summary_table/spacers_blastout_filt_complete.tsv",
       blastout_formatted="../results/host_assigniation/summary_table/spacers_blastout_filt_formatted.tsv",
       spacers_host="../results/host_assigniation/summary_table/spacers_host.tsv",
       prophages_host="../results/host_assigniation/summary_table/prophages_host.tsv",
       phage_host="../results/host_assigniation/summary_table/phage_host.tsv"
    log:
        "logs/assign_host/parse_host_assignation.log"
    conda:
        "envs/base_R_env.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "00:30:00"
    script:
        "scripts/assign_host/parse_host_assignation.R" # TODO run again

rule parse_vcontact_ntw:
    input:
        formatted_spacers="../results/host_assigniation/summary_table/spacers_metadata.tsv",
        phage_host="../results/host_assigniation/summary_table/phage_host.tsv",
        vcont_dir="../results/Vcontact2/vCONTACT_results"
    output:
        ph_vcont="../results/viral_classification/vcontact/phage_host_vcontact.tsv",
        vcont_ntw="../results/viral_classification/vcontact/vcontact_ntw.tsv",
        vcont_ntw_filt="../results/viral_classification/vcontact/vcontact_ntw_filtered.tsv"
    log:
        "logs/voiral_classification/vcontact/parse_vcontact_ntw.log"
    conda:
        "envs/base_R_env.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "00:30:00"
    script:
        "scripts/Viral_classification/parse_Vcontact2_ntw.R"

################################################################################
######################### PANGENOME ANALYISIS #########################
#Once our database is well curated, we can TODO
###############################################################################


################################## BACTERIA ####################################

# rule find_genomes_cluster:
#     input:
#         clust_info="../results/reference_db_filtered/summary_data_tables/clust_filtered.tsv",
#         filtered_mags="../results/MAG_binning/bins/filtered_mags/",
#         refs_isolates="../data/reference_assemblies/hb_bacteria/non_redundant/single_genomes/contigs/"
#     output:
#         paths="../results/pangenomes/{type}/{genus}_paths.txt",
#         tmp_db=temp(directory("../scratch_link/tmp_ref/reference_genomes_{type}_tmp_{genus}/"))
#     log:
#         "logs/{type}_pangenomes/{genus}/find_cluster_paths.log"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 2000,
#         runtime= "00:30:00"
#     shell:
#         "mkdir -p {output.tmp_db}; "
#         "cp {input.filtered_mags}/*.fa {output.tmp_db}; "
#         "cp {input.refs_isolates}/*.fna {output.tmp_db}; "
#         "python scripts/bacteria_pangenome/find_cluster_genomes.py {wildcards.genus} {output.tmp_db} {input.clust_info} {output.paths}"


rule annotate_bacterial_genomes:
    input:
        genomes="../scratch_link/reference_genomes_redundant"
    output:
        fnas=directory("../results/pangenomes/B/annotations/genes_fna"),
        faas=directory("../results/pangenomes/B/annotations/genes_faa")
    conda:
        "envs/orthofinder.yaml"
    log:
        "logs/B_pangenomes/prodiagal_annot.log" 
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 10000,
        runtime= "03:00:00"
    shell:
        "scripts/bacteria_pangenome/run_prod_annot_clusters.sh {output.fnas} {output.faas} {input.genomes}"



# rule annotate_clusters:
#     input:
#         genus_paths="../results/pangenomes/{type}/{genus}_paths.txt",
#         tmp_db="../scratch_link/tmp_ref/reference_genomes_{type}_tmp_{genus}/"
#     output:
#         fnas=directory("../results/pangenomes/{type}/{genus}/genes_fna"),
#         faas=directory("../results/pangenomes/{type}/{genus}/genes_faa")
#     conda:
#         "envs/orthofinder.yaml"
#     log:
#         "logs/{type}_pangenomes/{genus}/prodiagal_annot.log"
#     resources:
#         account = "pengel_beemicrophage",
#         mem_mb = 10000,
#         runtime= "00:30:00"
#     shell:
#         "mkdir -p {output.faas}; "
#         "mkdir -p {output.fnas}; "
#         "scripts/bacteria_pangenome/run_prod_annot_clusters.sh {output.fnas} {output.faas} {input.genus_paths}"

rule run_orthofinder:
    input:
        faas="../results/pangenomes/B/annotations/genes_faa",
    output:
        ortho_out=directory("../scratch_link/pangenomes/B/orthofinder_output/")
    conda:
        "envs/orthofinder.yaml"
    log:
        "logs/B_pangenomes/run_orthofinder.log"
    params:
        name="results",
        ulim=2000000
    threads: 46
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "72:00:00"
    shell:
        "ulimit -n {params.ulim}; "
        "orthofinder  -f {input.faas} -o {output.ortho_out} -n {params.name} -t {threads} -M msa"

rule parse_orthofinder:
    input:
        indir="../results/pangenomes/B/orthofinder_output/",
        clust_info="../results/reference_db_filtered/summary_data_tables/clust_filtered.tsv"
    output:
        parsed="../results/pangenomes/B/parsed_orthofinder_output/summary_tables/Orthogroups_table.tsv"
    conda:
        "envs/mOTUpan.yaml"
    log:
        "logs/B_pangenomes/parse_orthofinder.log"
    threads: 3
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "00:30:00"
    shell:
        "python scripts/pangenomes/parse_orthofinder.py -i {input.indir}/Results_results/Orthogroups -c {input.clust_info} -o {output.parsed}"

rule parse_orthofinder_genus:
    input:
        parsed="../results/pangenomes/B/parsed_orthofinder_output/summary_tables/Orthogroups_table.tsv"
    output:
        genus_parsed=directory("../results/pangenomes/B/parsed_orthofinder_output/{genus}/")
    conda:
        "envs/mOTUpan.yaml"
    log:
        "logs/B_pangenomes/parse_orthofinder_{genus}.log"
    threads: 3
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "00:30:00"
    shell:
        "python scripts/pangenomes/parse_orthofinder_genus.py -i {input.parsed} -g {wildcards.genus} -o {output.genus_parsed}"

rule run_mOTUpan:
    input:
        indir="../results/pangenomes/B/parsed_orthofinder_output/{genus}",
        ref_db="../results/reference_db"
    output:
        pan="../results/pangenomes/B/mOTUpan/{genus}_pan.tsv",
        chemck_out=temp("../results/pangenomes/B/mOTUpan/" + "{genus}_checkm_out_tab.tsv")
    conda:
        "envs/mOTUpan.yaml"
    log:
        "logs/B_pangenomes/B/{genus}_mOTUpan.log"
    benchmark:
        "logs/B_pangenomes/B/{genus}_mOTUpan.benchmark"
    threads: 15
    params:
        boots=1000,
        tmp1="../results/pangenomes/B/{genus}_tmp1.tsv",
        tmp2="../results/pangenomes/B/{genus}_tmp2.tsv"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 100000,
        runtime= "03:00:00"
    shell:
        """awk -F',' -v OFS=',' '{{ gsub(/.fa/,"", $1); print }} ' {input.ref_db}/data_tables/Chdb.csv > {params.tmp1}; """
        """awk -F',' -v OFS=',' '{{ gsub(/.fna/,"", $1); print }} ' {params.tmp1} > {params.tmp2}; """
        "sed -e 's/,/\t/g' {params.tmp2} > {output.chemck_out}; "
        "rm {params.tmp1} {params.tmp2}; "
        "mOTUpan.py -c {input.indir}/{wildcards.genus}_mOTUpan_table.tsv  -o {output.pan} --boots {params.boots} --checkm {output.chemck_out} --threads {threads}"


# rule single_OGs_diversity_parse:
#     input:
#         og="../scratch_link/pangenomes/{type}/{genus}/orthofinder_output/",
#         clust_info="../results/reference_db_filtered/summary_data_tables/clust_filtered.tsv"
#     output:
#         genus_OGs="../results/pangenomes/{type}/{genus}/{genus}_single_copy_OGs.tsv",
#         isolates_OGs="../results/pangenomes/{type}/{genus}/isolates_{genus}_single_copy_OGs.tsv"
#     log:
#         "logs/{type}_pangenomes/{genus}/parse_singOG_isolates.log"
#     params:
#         genus=lambda wildcards: wildcards.genus
#     threads: 3
#     conda:
#         "envs/drep_env.yaml"
#     resources:
#         account="pengel_beemicrophage",
#         mem_mb=50000,
#         runtime="00:30:00"
#     script:
#         "scripts/bacteria_pangenome/Find_genus_singleCopy_OGs.py"

# rule single_OGs_diversity_aggregate:
#     input:
#         genus_OGs=expand("../results/pangenomes/{{type}}/{genus}/{genus}_single_copy_OGs.tsv", genus=core_genera),
#         isolates_OGs=expand("../results/pangenomes/{{type}}/{genus}/isolates_{genus}_single_copy_OGs.tsv", genus=core_genera)
#     output:
#         all_genus_OGs="../results/pangenomes/{type}/all_single_copy_OGs.tsv",
#         all_isolates_OGs="../results/pangenomes/{type}/all_isolates_single_copy_OGs.tsv"
#     log:
#         "logs/{type}_pangenomes/aggregate_singOG_isolates.log"
#     threads: 3
#     resources:
#         account="pengel_beemicrophage",
#         mem_mb=5000,
#         runtime="00:30:00"
#     shell:
#         "echo -e 'Orthogroup\tgenome\tgene\tBin_Id\tsecondary_cluster\tspecies\tgenus\tsecondary_cluster_n' > {output.all_genus_OGs}; "
#         "echo -e 'Orthogroup\tgenome\tgene\tBin_Id\tsecondary_cluster\tspecies\tgenus\tsecondary_cluster_n' > {output.all_isolates_OGs}; "
#         "awk 'FNR>1' {input.genus_OGs} >> {output.all_genus_OGs}; "
#         "awk 'FNR>1' {input.isolates_OGs} >> {output.all_isolates_OGs}"



################################## Defense Systems ####################################

rule DF_launch:
    input:
        faa="../results/pangenomes/B/annotations/genes_faa/" + "{genome}_genes.faa"
    output:
        outdir=temp(directory("../results/pangenomes/B/defense_systems/{genome}"))
    conda:
        "envs/DF_env.yaml"
    threads: 5
    params:
        model="resources/other_db/defensefinder_models"
    log:
        "logs/B_pangenomes/defense_systems/{genome}_df.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 10000,
        runtime= "01:00:00"
    shell:
        """
        defense-finder run {input.faa} --out-dir {output} --models-dir {params.model} -w {threads} 
        """

rule DF_parse:
    input:
        df_out_genes=expand("../results/pangenomes/B/defense_systems/{genome}/defense_finder_genes.tsv",  genome=GENOMES),
        df_out_hmmer=expand("../results/pangenomes/B/defense_systems/{genome}/defense_finder_hmmer.tsv",  genome=GENOMES),
        df_out_systems=expand("../results/pangenomes/B/defense_systems/{genome}/defense_finder_systems.tsv",  genome=GENOMES)
    output:
        genes="../results/pangenomes/B/defense_systems/defense_finder_genes.tsv",
        hmmer="../results/pangenomes/B/defense_systems/defense_finder_hmmer.tsv",
        systems="../results/pangenomes/B/defense_systems/defense_finder_systems.tsv"
    log:
        "logs/B_pangenomes/defense_systems/parse_df.log"
    threads: 1
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 1000,
        runtime= "00:30:00"
    run:
        genes=concat_tables(input.df_out_genes, add={"genome": GENOMES})
        hmmer=concat_tables(input.df_out_hmmer, add={"genome": GENOMES})
        systems=concat_tables(input.df_out_systems, add={"genome": GENOMES})

        # save dataframes to output
        genes.to_csv(output.genes, sep="\t", index=False)
        hmmer.to_csv(output.hmmer, sep="\t", index=False)
        systems.to_csv(output.systems, sep="\t", index=False)



################################## Phages ####################################







################################################################################
######################### MAP TO REFERENCE DB ##################################
#Once our database is well curated, we can TODO
################################################################################

rule split_drep_phages:
    input:
        genomes="../results/assembly/viral_contigs/dereplicated/all_viral_contigs_drep.fasta"
    output:
        single_genomes_drep=directory("../scratch_link/reference_phages_drep/")
    log:
        "logs/ref_db/viruses/split_drep.log"
    threads: 2
    conda:
        "envs/drep_env.yaml"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 1500,
        runtime= "00:30:00"
    shell:
        "python scripts/assembly/split_assembly.py -f {input.genomes} -d {output.single_genomes_drep}"


############################### BACTERIA #######################################
rule get_stb_bacteria:
    input:
        refs="../results/reference_db_filtered/dereplicated_genomes_filtered/"
    output:
        concat="../results/reference_db_filtered/B/all_B_RefGenomes.fasta",
        stb="../results/reference_db_filtered/B/all_B_RefGenomes.stb"
    log:
        "logs/ref_db/concat_refs.log"
    threads: 2
    conda:
        "envs/drep_env.yaml"
    params:
        refs=lambda wildcards, input: get_files_commas(input[0], sep=" ")
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 1500,
        runtime= "00:30:00"
    shell:
        "cat {input.refs}/*.f* >> {output.concat}; "
        "parse_stb.py --reverse -f {params.refs}  -o {output.stb}"

rule get_stb_viruses:
    input:
        single_genomes_drep="../scratch_link/reference_phages_drep/"
    output:
        concat="../results/reference_db_filtered/P/all_P_RefGenomes.fasta",
        stb="../results/reference_db_filtered/P/all_P_RefGenomes.stb" 
    log:
        "logs/ref_db/viruses/stb.log"
    threads: 2
    conda:
        "envs/drep_env.yaml"
    params:
        refs=lambda wildcards, input: get_files_commas(input[0], sep=" ")
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 1500,
        runtime= "00:30:00"
    shell:
        "cat {input.single_genomes_drep}/*.f* >> {output.concat}; "
        "parse_stb.py --reverse -f {params.refs}  -o {output.stb}"


rule build_ref_index:
    input:
        ref="../results/reference_db_filtered/{type}/all_{type}_RefGenomes.fasta",
    output:
        index=directory("../results/reference_db_filtered/{type}_index/")
    conda:
        "envs/map_env.yaml"
    threads: 25
    log:
        "logs/mapping/build_bowtie_{type}_index.log"
    params:
        basename="all_{type}_RefGenomes",
    resources:
        account= "pengel_beemicrophage",
        mem_mb= 20000,
        runtime= "01:00:00"
    shell:
        "mkdir -p {output.index}; "
        "bowtie2-build {input.ref} {output.index}/{params.basename} --threads {threads}"

rule MapReads:
    input:
        index="../results/reference_db_filtered/{type}_index/",
        R1="../data/host_filtered_reads/{sample}{type}_R1_HF.fastq.gz",
        R2="../data/host_filtered_reads/{sample}{type}_R2_HF.fastq.gz",
    output:
        sam=temp("../scratch_link/mapping/mapdata_{type}/{sample}{type}_mapping.sam"),
    wildcard_constraints:
        sample="\d+"
    conda:
        "envs/map_env.yaml"
    threads: 25
    params:
        basename="all_{type}_RefGenomes"
    log:
        "logs/mapping/map/{type}/{sample}{type}_bbmap_mapping.log"
    benchmark:
        "logs/mapping/map/{type}/{sample}{type}_bbmap_mapping.benchmark"
    resources:
        account= "pengel_beemicrophage",
        mem_mb= 200000,
        runtime= "02:00:00"
    shell:
        "bowtie2 -x {input.index}/{params.basename} -1 {input.R1} -2 {input.R2} -S {output.sam} --very-sensitive --threads {threads}"

rule generate_bam:
    input:
        sam="../scratch_link/mapping/mapdata_{type}/{sample}{type}_mapping.sam"
    output:
        bam="../scratch_link/mapping/mapdata_{type}/{sample}{type}_mapping.bam",
        cf=temp("../results/mapping/{sample}{type}_library_count.tsv")
    wildcard_constraints:
        sample="\d+"
    conda:
        "envs/map_env.yaml"
    threads: 2
    log:
        "logs/mapping/map/{type}/{sample}{type}_generateBAM.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 20000,
        runtime= "01:00:00"
    shell:
        "samtools view -S -b {input.sam} > {output.bam}; "
        "touch {output.cf}; "
        "s=$(basename {output.bam}); "
        "lib=${{s%.*}}; "
        "count=$(samtools view -c {output.bam}); "
        'echo -e "${{lib}}\t${{count}}" >> {output.cf}'

################################################################################
################################## INSTRAIN ####################################
################################################################################
rule annotate_viral_genomes_reference:
    input:
        genomes="../scratch_link/reference_phages_drep/"
    output:
        fnas=directory("../results/pangenomes/P/annotations_reference/genes_fna"),
        faas=directory("../results/pangenomes/P/annotations_reference/genes_faa")
    conda:
        "envs/orthofinder.yaml"
    log:
        "logs/P_pangenomes/prodiagal_annot.log" 
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 10000,
        runtime= "02:00:00"
    shell:
        "scripts/bacteria_pangenome/run_prod_annot_clusters.sh {output.fnas} {output.faas} {input.genomes}"

BACTERIA_REFS1=glob_wildcards("../results/reference_db_filtered/dereplicated_genomes_filtered/{genome}.fna")
BACTERIA_REFS2=glob_wildcards("../results/reference_db_filtered/dereplicated_genomes_filtered/{genome}.fa")

rule generate_genelist_bacterial:
    input:
        ref=expand("../results/pangenomes/B/annotations/genes_fna/" + "{genome}_genes.fna", genome=BACTERIA_REFS1.genome + BACTERIA_REFS2.genome)
    output:
        "../results/inStrain/all_B_RefGenomes_genes.fna"
    conda:
        "envs/inStrain.yaml"
    threads: 2
    log:
        "logs/instrain/generate_gene_list_B.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 20000,
        runtime= "1:00:00"
    shell:
        "cat {input.ref} > {output}"

rule generate_genelist_viral:
    input:
        ref="../results/pangenomes/P/annotations_reference/genes_fna"
    output:
        "../results/inStrain/all_P_RefGenomes_genes.fna"
    log:
        "logs/instrain/generate_gene_list_P.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 20000,
        runtime= "00:10:00"
    shell:
        "cat {input.ref}/* > {output}"


rule instrain_profile:
    input:
        bam="../scratch_link/mapping/mapdata_{type}/{sample}{type}_mapping.bam",
        ref="../results/reference_db_filtered/{type}/all_{type}_RefGenomes.fasta",
        genL="../results/inStrain/all_{type}_RefGenomes_genes.fna",
        stb="../results/reference_db_filtered/{type}/all_{type}_RefGenomes.stb" 
    output:
        dir=directory("../results/inStrain/profile_{type}/{sample}{type}_profile/")
    threads: 32
    conda:
        "envs/inStrain.yaml"
    log:
        "logs/instrain/{sample}{type}_profile.log"
    benchmark:
        "logs/instrain/{sample}{type}_profile.benchmark"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 700000,
        runtime= "10:00:00"
    params:
        min_ANI=0.92
    shell:
        "(inStrain profile {input.bam} {input.ref} -o {output.dir} --min_read_ani {params.min_ANI} -p {threads} -g {input.genL} -s {input.stb})2> {log}"

rule get_gene_info_bacterial:
    input:
        instrain="../results/inStrain/profile_B/{sample}B_profile/",
        pans=expand("../results/pangenomes/B/mOTUpan/{genus}_pan.tsv", genus=GENUSES),
        orthogroup_tab="../results/pangenomes/B/parsed_orthofinder_output/summary_tables/Orthogroups_table.tsv"
    output:
        genes="../results/inStrain/profile_B/tmp_data/{sample}B_gene_info.tsv",
        core="../results/inStrain/profile_B/tmp_data/{sample}B_core_gene_info.tsv"
    conda:
        "envs/mOTUpan.yaml"
    log:
        "logs/instrain/summarize/{sample}B_gene_info.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 50000,
        runtime= "00:15:00"
    shell:
        "python scripts/bacteria_community_analysis/core_genome_parse.py -i {input.orthogroup_tab} -m {input.pans} -g {input.instrain}/output/{wildcards.sample}B_profile_gene_info.tsv -o {output.genes} -c {output.core}"

rule aggregate_instrain_bacterial:
    input:
        IS=expand("../results/inStrain/profile_B/{sample}B_profile", sample=config["sam_names"], type=type_of),
        genes=expand("../results/inStrain/profile_B/tmp_data/{sample}B_gene_info.tsv", sample=config["sam_names"]),
        tax="../results/reference_db_filtered/summary_data_tables/clust_filtered_winners.tsv"
    output:
        outdir=directory("../results/inStrain/profile_B/summary_data/")
    conda:
        "envs/mOTUpan.yaml"
    log:
        "logs/instrain/summarize/aggreagte_profile_data_tabs.log"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 50000,
        runtime= "00:30:00"
    shell:
        "python scripts/bacteria_community_analysis/parse_instrain_bacterial.py -i {input.IS} -g {input.genes} -t {input.tax} -o {output.outdir}"

rule do_rarefaction:
    input:
        IS=expand("../scratch_link/inStrain/profile_{{type}}/{sample}{{type}}_profile", sample=config["sam_names"]),
        clust_info="../results/reference_db_filtered/summary_data_tables/clust_filtered_winners.tsv",
        clust_filt="../results/reference_db_filtered/summary_data_tables/clust_filtered.tsv",
        genome_info="../results/inStrain/{type}/data_tables/all_genome_info.tsv",
        gene_info="../results/inStrain/{type}/data_tables/all_gene_info.tsv",
        stb="../results/reference_db_filtered/all_genomes/all_bacterial_RefGenomes.stb",
        motupan=expand("../results/pangenomes/{{type}}/{genus}/{genus}_mOTUpan.tsv", genus=core_genera)
    output:
        rare="../results/inStrain/{type}/data_tables/snv_counts_mOTUpanCore.tsv"
    conda:
        "envs/rare_R.yaml"
    params:
        us_func="scripts/useful_func.R"
    log:
        "logs/instrain/{type}/do_rarefaction.log"
    threads: 40
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 200000,
        runtime= "01:00:00"
    script:
        "scripts/bacteria_community_analysis/do_rarefaction.R"

rule instrain_compare_bact:
    input:
        IS=expand("../results/inStrain/profile_B/{sample}B_profile/", sample=config["sam_names"]),
        ref="../results/reference_db_filtered/B/all_B_RefGenomes.fasta",
        stb="../results/reference_db_filtered/B/all_B_RefGenomes.stb" 
    output:
        directory("../results/inStrain/compare_B/compare_B/compare_{genome}")
    threads: 40
    conda:
        "envs/inStrain.yaml"
    log:
        "logs/instrain/compare/B_compare_{genome}.log"
    benchmark:
        "logs/instrain/compare/B_compare_{genome}.benchmark"
    resources:
        account = "pengel_beemicrophage",
        mem_mb = 600000,
        runtime= "3-00:00:00"
    params:
        genome=lambda wildcards: wildcards.genome
    shell:
        "inStrain compare -i {input.IS} -o {output} -p {threads} -s {input.stb} --database_mode --genome {params.genome} || mkdir -p {output}/singleton"
        # Since I feed all the genomes of the rference database to this command, if the genom is not present in more than on sample, the command will fail.
        # As a solution, I catch the error and make the output directory anyways to trick snakemake
        # This is not the best solution. TODO make something more elegeant